---
created: 2022-11-16T11:33:13 (UTC -08:00)
tags: []
source: https://www.currentaffairs.org/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk
author: Current Affairs
---

# The Dangerous Ideas of “Longtermism” and “Existential Risk” ❧ Current Affairs

> ## Excerpt
> <p>So-called rationalists have created a disturbing secular religion that looks like it addresses humanity’s deepest problems, but actually justifies pursuing the social preferences of elites.</p>

---
In a late-2020 [interview](https://www.cnbc.com/2020/12/29/skype-co-founder-jaan-tallinn-on-3-most-concerning-existential-risks-.html) with CNBC, Skype cofounder Jaan Tallinn made a perplexing statement. “Climate change,” he said, “is not going to be an existential risk unless there’s a runaway scenario.” A “runaway scenario” would occur if crossing one or more critical thresholds in the climate system causes Earth’s thermostat to rise uncontrollably. The hotter it _has_ become, the hotter it _will_ become, via self-amplifying processes. This is probably [what happened](https://theconversation.com/venus-was-once-more-earth-like-but-climate-change-made-it-uninhabitable-150445) a few billion years ago on our planetary neighbor Venus, a hellish cauldron whose average surface temperature is high enough to melt lead and zinc.

Fortunately, the best science today suggests that a runaway scenario is [unlikely](https://theconversation.com/climate-explained-rising-carbon-emissions-probably-wont-make-the-earth-uninhabitable-155447), although not impossible. Yet even without a runaway scenario, the best science also frighteningly affirms that climate change will have devastating consequences. It will [precipitate](https://docs.wixstatic.com/ugd/d9aaad_b2e7f0f56bec40a195e551dd3e8c878e.pdf) lethal heatwaves, megadroughts, catastrophic wildfires (like those seen recently in the Western U.S.), desertification, sea-level rise, mass migrations, widespread political instability, food-supply disruptions/famines, extreme weather events (more dangerous hurricanes and flash floods), infectious disease outbreaks, biodiversity loss, mass extinctions, ecological collapse, socioeconomic upheaval, [terrorism and wars](https://slate.com/technology/2015/03/study-climate-change-helped-spark-syrian-civil-war.html), etc. To quote an ominous [2020 paper](https://academic.oup.com/bioscience/article/70/1/8/5610806) co-signed by more than 11,000 scientists from around the world, “planet Earth is facing a climate emergency” that, unless immediate and drastic action is taken, will bring about “untold suffering.”

So why does Tallinn think that climate change _isn’t_ an existential risk? Intuitively, if anything should count as an existential risk it’s climate change, right?

Cynical readers might suspect that, given Tallinn’s immense fortune of an [estimated $900 million](https://fortune.com/2020/11/13/jaan-tallinn-ai-safety-bitcoin-cryptocurrency-elon-musk/), this might be just another case of a super-wealthy tech guy dismissing or minimizing threats that probably won’t directly harm _him personally_. Despite being [disproportionately responsible](https://www.cnbc.com/2021/01/26/oxfam-report-the-global-wealthy-are-main-drivers-of-climate-change.html) for the climate catastrophe, the [super-rich](https://www.technologyreview.com/2018/03/01/144958/if-youre-so-smart-why-arent-you-rich-turns-out-its-just-chance/) will be the least affected by it. Peter Thiel—the libertarian who voted for a [climate-denier](https://www.politifact.com/factchecks/2016/jun/03/hillary-clinton/yes-donald-trump-did-call-climate-change-chinese-h/) in 2016—has his “[apocalypse retreat](https://www.theguardian.com/news/2018/feb/15/why-silicon-valley-billionaires-are-prepping-for-the-apocalypse-in-new-zealand)” in New Zealand, Richard Branson owns his own [hurricane-proof island](https://www.businessinsider.com/hurricane-irma-richard-branson-private-island-bunker-2017-9), Jeff Bezos bought some [400,000 acres](https://africa.businessinsider.com/finance-billionaires-are-stockpiling-land-that-could-be-used-in-the-apocalypse-heres/pmfybd4) in Texas, and Elon Musk wants to [move to Mars](https://www.extremetech.com/extreme/318959-elon-musk-richest-man-mars-colony). Astoundingly, Reid Hoffman, the multi-billionaire who cofounded LinkedIn, [reports](https://www.imd.org/research-knowledge/articles/what-techs-survivalist-billionaires-should-be-doing-instead/) that “more than 50 percent of Silicon Valley’s billionaires have bought some level of ‘apocalypse insurance,’ such as an underground bunker.”

That’s one possibility, for sure. But I think there’s a deeper reason for Tallinn’s comments. It concerns an increasingly influential moral worldview called _longtermism_. This has roots in the work of philosopher Nick Bostrom, who [coined](https://nickbostrom.com/existential/risks.html) the term “existential risk” in 2002 and, three years later, founded the Future of Humanity Institute (FHI) based at the University of Oxford, which has received large sums of money from both [Tallinn](https://fortune.com/2020/11/13/jaan-tallinn-ai-safety-bitcoin-cryptocurrency-elon-musk/) and [Musk](https://www.fhi.ox.ac.uk/elon-musk-funds-oxford-and-cambridge-university-research-on-safe-and-beneficial-artificial-intelligence/). Over the past decade, “longtermism” has become one of the main ideas promoted by the “[Effective](https://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai) [Altruism](https://www.lrb.co.uk/the-paper/v37/n18/amia-srinivasan/stop-the-robot-apocalypse)” (EA) movement, which generated controversy in the past for encouraging young people to work for [Wall Street](https://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai) and [petrochemical companies](https://assets.ctfassets.net/es8pp29e1wp8/4C7WHsxZLWeaQgiIksMS0y/73404620d8c355f94f7d4d1130e967e8/Gabriel_published_14_April_3.pdf) in order to donate part of their income to charity, an idea called “earn to give.” According to the longtermist Benjamin Todd, formerly at Oxford University, “[longtermism](https://80000hours.org/articles/future-generations/) might well turn out to be one of the most important discoveries of effective altruism so far.”

Longtermism should not be confused with “long-term thinking.” It goes _way beyond_ the observation that our society is dangerously myopic, and that we should care about future generations no less than present ones. At the heart of this worldview, as delineated by Bostrom, is the idea that what _matters most_ is for “Earth-originating intelligent life” to fulfill its _potential_ in the cosmos. What exactly is “our potential”? As I have [noted elsewhere](https://c8df8822-f112-4676-8332-ad89713358e3.filesusr.com/ugd/d9aaad_89094654cf0945738f5633b5d46653fd.pdf), it [involves](https://www.existential-risk.org/concept.html) subjugating nature, maximizing economic productivity, replacing humanity with a superior “posthuman” species, colonizing the universe, and ultimately creating an unfathomably huge population of conscious beings living what Bostrom [describes](https://www.google.com/search?tbm=bks&q=%2522rich+and+happy+lives%2522+%2522nick+bostrom%2522) as “rich and happy lives” inside high-resolution computer simulations.

This is what “our potential” consists of, and it constitutes the ultimate aim toward which humanity as a whole, and each of us as [individuals](https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf), are morally obligated to strive. An _existential risk_, then, is any event that would destroy this “[vast and glorious](https://www.google.com/search?tbm=bks&q=%2522vast+and+glorious%2522+%2522the+precipice%2522)” potential, as Toby Ord, a philosopher at the Future of Humanity Institute, [writes](https://www.google.com/search?tbm=bks&q=%2522destruction+of+our+longterm+potential%2522+%2522toby+ord%2522) in his 2020 book _The Precipice_, which draws heavily [from](https://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501/ref=sr_1_1?dchild=1&keywords=global+catastrophic+risks&qid=1626689935&sr=8-1) [earlier](https://www.amazon.com/Here-Be-Dragons-Technology-Humanity-ebook/dp/B018ZK17UI/ref=sr_1_1?dchild=1&keywords=here+be+dragons+haggstrom&qid=1626609967&sr=8-1) [work](https://www.amazon.com/Morality-Foresight-Human-Flourishing-Introduction/dp/1634311426/ref=sr_1_15?dchild=1&keywords=morality+foresight&qid=1626609986&sr=8-15) in outlining the longtermist paradigm. (Note that Noam Chomsky just published a [book](https://www.amazon.com/Precipice-Neoliberalism-Pandemic-Urgent-Radical/dp/164259458X/ref=sr_1_1?dchild=1&keywords=the+precipice&qid=1625949590&sr=8-1) also titled _The Precipice_.)

The point is that when one takes the cosmic view, it becomes clear that our civilization could persist for an _incredibly long time_ and there could come to be an _unfathomably large number_ of people in the future. Longtermists thus reason that the far future could contain _way more value_ than exists today, or has existed so far in human history, which stretches back some 300,000 years. So, imagine a situation in which you could either lift 1 billion present people out of extreme poverty _or_ benefit 0.00000000001 percent of the 10<sup>23</sup> biological humans who Bostrom [calculates](https://www.nickbostrom.com/astronomical/waste.html) could exist if we were to colonize our cosmic neighborhood, the Virgo Supercluster. Which option should you pick? For longtermists, the answer is obvious: you should pick the latter. Why? Well, just crunch the numbers: 0.00000000001 percent of 10<sup>23</sup> people is 10 billion people, which is _ten times greater_ than 1 billion people. This means that if you want to do the _most good_, you should focus on these far-future people rather than on helping those in extreme poverty today. As the FHI longtermists Hilary Greaves and Will MacAskill—the latter of whom is [said to have](https://www.simonknutsson.com/problems-in-effective-altruism-and-existential-risk-and-what-to-do-about-them/%23Potentially_dishonest_self-promotion) cofounded the Effective Altruism movement with Toby Ord—[write](https://globalprioritiesinstitute.org/wp-content/uploads/2019/Greaves_MacAskill_The_Case_for_Strong_Longtermism.pdf), “for the purposes of evaluating actions, we can in the first instance often _simply ignore_ all the effects contained in the first 100 (or even 1,000) years, focussing primarily on the further-future effects. Short-run effects act as little more than tie-breakers.”

___

This brings us back to climate change, which is expected to cause serious harms over precisely this time period: the next few decades and centuries. If what matters most is the very _far future_—thousands, millions, billions, and trillions of years from now—then climate change isn’t going to be high up on the list of global priorities _unless there’s a runaway scenario_. Sure, it will cause “untold suffering,” but think about the situation from the point of view of the universe itself. Whatever traumas and miseries, deaths and destruction, happen this century will _pale in comparison_ to the astronomical amounts of “value” that could exist once humanity has colonized the universe, become posthuman, and created upwards of 10<sup>58</sup> (Bostrom’s later [estimate](https://www.google.com/books/edition/Superintelligence/7_H8AwAAQBAJ?hl=en&gbpv=1&dq=%22human+lives+could+be+created+in+emulation+even+with+quite+conservative%22&pg=PA103&printsec=frontcover)) conscious beings in computer simulations. Bostrom [makes this point](https://www.nickbostrom.com/papers/future.html) in terms of economic growth, which he and [other longtermists](https://forum.effectivealtruism.org/posts/sFj7EstDYacf6GJWF/q-and-a-with-will-macaskill) see as integral to fulfilling “our potential” in the universe:

> “_In absolute terms, \[non-runaway climate change\] would be a huge harm. Yet over the course of the twentieth century, world GDP grew by some 3,700%, and per capita world GDP rose by some 860%. It seems safe to say that … whatever negative economic effects global warming will have, they will be completely swamped by other factors that will influence economic growth rates in this century.”_

In the same paper, Bostrom [declares](https://www.nickbostrom.com/papers/future.html) that even “a non-existential disaster causing the breakdown of global civilization is, from the perspective of humanity as a whole, a potentially recoverable setback,” describing this as “a giant massacre for man, a small misstep for mankind.” That’s of course cold comfort for those in the crosshairs of climate change—the residents of the Maldives who will [lose their homeland](https://www.cnbc.com/2021/05/19/maldives-calls-for-urgent-action-to-end-climate-change-sea-level-rise.html%23:~:text=Capital%2520Connection-,The%2520Maldives%2520could%2520disappear%2520by%2520the%2520end%2520of%2520the%2520century,environment,%2520climate%2520change%2520and%2520technology.&text=and%2520the%2520sea.%25E2%2580%259D-,The%2520World%2520Economic%2520Forum%2520has%2520estimated%2520that%2520by%25202050,%252080,be%2520impacted%2520by%2520climate%2520change.), the South Asians facing [lethal heat waves](https://news.mit.edu/2017/deadly-heat-waves-could-hit-south-asia-century-0802) above the 95-degree F wet-bulb threshold of survivability, and the 18 million people in Bangladesh who [may be displaced](https://ejfoundation.org/reports/climate-displacement-in-bangladesh%23:~:text=Climate%2520Change%2520in%2520Bangladesh,exceptionally%2520vulnerable%2520to%2520climate%2520change.&text=It%2520has%2520been%2520estimated%2520that,of%2520sea%2520level%2520rise%2520alone.) by 2050. But, once again, when these losses are juxtaposed with the apparent immensity of our longterm “potential,” this suffering will hardly be a footnote to a footnote within humanity’s epic biography.

These aren’t the only incendiary remarks from Bostrom, the Father of Longtermism. In a paper that founded one half of longtermist research program, he [characterizes](https://nickbostrom.com/existential/risks.html) the most devastating disasters throughout human history, such as the two World Wars (including the Holocaust), Black Death, 1918 Spanish flu pandemic, major earthquakes, large volcanic eruptions, and so on, as “mere ripples” when viewed from “the perspective of humankind as a whole.” As he writes: 

> _“Tragic as such events are to the people immediately affected, in the big picture of things … even the worst of these catastrophes are mere ripples on the surface of the great sea of life.”_ 

In other words, 40 million civilian deaths during WWII was awful, we can all agree about that. But think about this in terms of the 10<sup>58</sup> simulated people who could someday exist in computer simulations if we colonize space. It would require _trillions and trillions and trillions_ of WWIIs one after another to even _approach_ the loss of these unborn people if an existential catastrophe were to happen. This is the case even on the lower estimates of how many future people there could be. Take Greaves and MacAskill’s [figure](https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf) of 10<sup>18</sup> expected biological and digital beings on Earth alone (meaning that we don’t colonize space). That’s still a _way_ bigger number than 40 million—analogous to a single grain of sand next to Mount Everest.

It’s this line of reasoning that leads Bostrom, Greaves, MacAskill, and others to argue that even the _tiniest_ reductions in “existential risk” are _morally equivalent_ to saving the lives of literally _billions_ of living, breathing, actual people. For example, Bostrom [writes](https://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai) that if there is “a mere 1 percent chance” that 10<sup>54</sup> conscious beings (most living in computer simulations) come to exist in the future, then “we find that the expected value of reducing existential risk by a mere _one billionth of one billionth of one percentage point_ is worth a hundred billion times as much as a billion human lives.” Greaves and MacAskill [echo this idea](https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf) in a 2021 paper by arguing that “even if there are ‘only’ 10<sup>14</sup> lives to come … , a reduction in near-term risk of extinction by one millionth of one percentage point would be equivalent in value to a million lives saved.”

To make this concrete, imagine Greaves and MacAskill in front of two buttons. If pushed, the first would save the lives of 1 million living, breathing, actual people. The second would increase the probability that 10<sup>14</sup> currently unborn people come into existence in the _far future_ by a _teeny-tiny_ amount. Because, on their longtermist view, there is no fundamental moral difference between saving _actual people_ and _bringing new people_ into existence, these options are _morally equivalent_. In other words, they’d have to flip a coin to decide which button to push. (Would you? I certainly hope not.) In Bostrom’s example, the morally right thing is _obviously_ to sacrifice billions of living human beings for the sake of even _tinier_ reductions in existential risk, assuming a minuscule 1 percent chance of a _larger_ future population: 10<sup>54</sup> people.

All of this is to say that even if billions of people were to perish in the coming climate catastrophe, so long as humanity survives with enough of civilization intact to fulfill its supposed “potential,” we shouldn’t be _too_ concerned. In the grand scheme of things, non-runaway climate change will prove to be nothing more than a “mere ripple” —a “small misstep for mankind,” however terrible a “massacre for man” it might otherwise be.

Even worse, since our resources for reducing existential risk are finite, Bostrom [argues](https://www.existential-risk.org/concept.html%23:~:text=An%2520existential%2520risk%2520is%2520one,future%2520development%2520(Bostrom%25202002).) that we must not “fritter \[them\] away” on what he describes as “feel-good projects of suboptimal efficacy.” Such projects would include, on this account, not just saving people in the Global South—those most vulnerable, especially [women](https://www.bbc.com/news/science-environment-43294221)—from the calamities of climate change, but _all other_ non-existential philanthropic causes, too. As the Princeton philosopher Peter Singer [writes](https://www.google.com/search?tbm=bks&q=%2522to+refer+to+donating+to+help+the+global+poor+or+reduce+animal+suffering+as+a+%25E2%2580%2598feel-good+project%25E2%2580%2599+on+which+resources+are+%25E2%2580%2598frittered+away%25E2%2580%2599+is+harsh+language%2522) about Bostrom in his 2015 book on Effective Altruism, “to refer to donating to help the global poor … as a ‘feel-good project’ on which resources are ‘frittered away’ is harsh language.” But it makes perfectly good sense within Bostrom’s longtermist framework, [according to which](https://www.nickbostrom.com/astronomical/waste.html) “priority number one, two, three, and four should … be to reduce existential risk.” Everything else is smaller fish not worth frying.

___

If this sounds appalling, it’s because it _is_ appalling. By reducing morality to an abstract [numbers game](https://users.ox.ac.uk/~mert2255/papers/mu-about-pe.pdf), and by declaring that what’s [most important](https://books.google.de/books?hl=en&lr=&id=iPerDwAAQBAJ&oi=fnd&pg=PA80&dq=overwhelming+importance+far+future+beckstead&ots=fJHd_shkvv&sig=BJ3_Xf-5XpwWmTcz5RvPTEJKOcg&redir_esc=y%23v=onepage&q&f=false) is fulfilling “our potential” by becoming simulated posthumans among the stars, longtermists not only trivialize past atrocities like WWII (and the Holocaust) but give themselves a “moral excuse” to dismiss or minimize comparable atrocities in the future. This is one reason that I’ve come to see longtermism as an _immensely dangerous ideology_. It is, indeed, akin to a _secular religion_ built around the worship of “future value,” complete with its own “secularised doctrine of salvation,” as the Future of Humanity Institute historian Thomas Moynihan approvingly [writes](https://www.google.com/search?tbm=bks&q=%2522secularised+doctrine+of+salvation%2522) in his book _X-Risk_. The popularity of this religion among wealthy people in the West—especially the socioeconomic elite—makes sense because it tells them exactly what they want to hear: not only are you _ethically excused_ from worrying too much about sub-existential threats like non-runaway climate change and global poverty, but you are actually a _morally better person_ for focusing instead on more important things—risk that could permanently destroy “our potential” as a species of Earth-originating intelligent life.

To drive home the point, consider an argument from the longtermist Nick Beckstead, who has [overseen](https://www.openphilanthropy.org/focus/global-catastrophic-risks/miscellaneous/future-humanity-institute-work-on-global-catastrophic-risks) tens of millions of dollars in funding for the Future of Humanity Institute. Since shaping the far future “over the coming millions, billions, and trillions of years” is of “overwhelming importance,” he [claims](https://www.proquest.com/docview/1442191960?pq-origsite=gscholar&fromopenview=true), we should actually care more about people in rich countries than poor countries. This comes from a 2013 PhD dissertation that Ord [describes](https://www.google.com/search?tbm=bks&q=%2522one+of+the+best+texts+on+existential+risk%2522) as “one of the best texts on existential risk,” and it’s cited on numerous Effective Altruist [websites](https://concepts.effectivealtruism.org/concepts/the-long-term-future/), including some hosted by the Centre for Effective Altruism, which shares office space in Oxford with the Future of Humanity Institute. The passage is worth quoting in full:

> “_Saving lives in poor countries may have significantly smaller ripple effects than saving and improving lives in rich countries. Why? Richer countries have substantially more innovation, and their workers are much more economically productive. By ordinary standards—at least by ordinary enlightened humanitarian standards—saving and improving lives in rich countries is about equally as important as saving and improving lives in poor countries, provided lives are improved by roughly comparable amounts. But it now seems more plausible to me that saving a life in a rich country is substantially more important than saving a life in a poor country, other things being equal.”_

Never mind the fact that many countries in the Global South are relatively poor precisely because of the long and sordid histories of Western colonialism, imperialism, exploitation, political meddling, pollution, and so on. What hangs in the balance is astronomical amounts of “value.” What shouldn’t we do to achieve this magnificent end? Why _not_ prioritize lives in rich countries over those in poor countries, even if gross historical injustices remain inadequately addressed? Beckstead isn’t the only longtermist who’s explicitly endorsed this view, either. As Hilary Greaves [states](https://youtu.be/d1jMlb8E08k?t=148) in a 2020 interview with Theron Pummer, who co-edited the book _Effective Altruism_ with her, if one’s “aim is doing the most good, improving the world by the most that I can,” then although “there’s a clear place for transferring resources from the affluent Western world to the global poor … _longtermist thought_ suggests that something else may be _better still.”_

___

Returning to climate change once again, we can see how Tallinn got the idea that our environmental impact probably isn’t existentially risky from academic longtermists like Bostrom. As alluded to above, Bostrom [maintains](https://nickbostrom.com/existential/risks.html) that non-runaway (which he calls “moderate”) global warming, as well as “threats to the biodiversity of Earth’s ecosphere,” as “endurable” rather than “terminal” for humanity. Similarly, Ord [claims](https://www.google.com/search?tbm=bks&q=%25221+in+10000%2522+%2522the+precipice%2522+%25221+in+10%2522) in _The Precipice_ that climate change poses a mere 1-in-1,000 chance of existential catastrophe, in contrast to a far greater 1-in-10 chance of catastrophe involving superintelligent machines (dubbed the “Robopocalypse” by [some](https://www.google.com/search?tbm=bks&q=%25E2%2580%259Ca+disaster+sometimes+called+the+Robopocalypse+and+commonly+illustrated+with+stills+from+the+Terminator+movies.%25E2%2580%259D)). Although, like Bostrom, Ord acknowledges that the climate crisis could get very bad, he [assures](https://youtu.be/R9EtiNmYnQQ?t=510) us that “the typical scenarios of climate change would not destroy our potential.”

Within the billionaire world, these conclusions have been parroted by some of the most powerful men on the planet today (not just Tallinn). For example, Musk, an admirer of Bostrom’s who donated $10 million in 2015 to the Future of Life Institute, another longtermist organization that Tallinn cofounded, [said](https://www.youtube.com/watch?v=Jvx_XIihmXs) in an interview this year that his “concern with the CO2 is not kind of where we are today or even … the current rate of carbon generation.” Rather, the worry is that “if carbon generation keeps accelerating and … if we’re complacent then I think … there’s some risk of sort of non-linear climate change”—meaning, one surmises, a runaway scenario. Peter Thiel has also apparently held this view for some time, which is unsurprising given his history with longtermist thinking and the Effective Altruism movement. (He gave the [keynote address](https://www.youtube.com/watch?v=h8KkXcBwHec&t=1439s) at the 2013 Effective Altruism Summit.) But Thiel also [declared](https://www.ft.com/content/abc942cc-5fb3-11e4-8c27-00144feabdc0) in 2014: “People are spending way too much time thinking about climate change” and “way too little thinking about AI.”

The reference to AI, or “artificial intelligence,” here is important. Not only do many longtermists believe that [superintelligent machines](https://www.currentaffairs.org/2020/07/the-singularity-prophets) pose the greatest single hazard to human survival, but they seem convinced that if humanity were to create a “friendly” superintelligence whose goals are properly “aligned” with our “human goals,” then a new Utopian age of unprecedented security and flourishing would suddenly commence. This eschatological vision is sometimes associated with the “Singularity,” made famous by futurists like Ray Kurzweil, which critics have facetiously dubbed the “[techno-rapture](https://www.google.com/search?tbm=bks&q=%2522Since+lucky+humans+will+at+that+point+merge+with+superintelligence+or+become+superintelligent,+some+refer+to+the+Singularity+as+the+%27Techno-rapture%27,+pointing+out+the+similarity+ofthe+narrative+to+the+Christian+Rapture%2522)” or “[rapture of the nerds](https://git.jrtechs.net/jrtechs/FOSSRIT-hfoss/raw/commit/6a8f78c0dc52892458a4f12d2e322a1d2e1df6ac/static/books/Cory_Doctorow_and_Charles_Stross_-_Rapture_of_the_Nerds.pdf)” because of its obvious similarities to the Christian dispensationalist notion of the Rapture, when Jesus will swoop down to gather every believer on Earth and carry them back to heaven. As Bostrom [writes](https://www.google.com/search?tbm=bks&q=%2522would+also+eliminate+or+reduce+many+anthropogenic+risks%2522) in his [Musk-endorsed](https://twitter.com/elonmusk/status/495759307346952192?lang=en) book _Superintelligence_, not only would the various existential risks posed by nature, such as asteroid impacts and supervolcanic eruptions, “be virtually eliminated,” but a friendly superintelligence “would also eliminate or reduce many anthropogenic risks” like climate change. “One might believe,” he [writes](https://www.nickbostrom.com/papers/future.html) elsewhere, that “the new civilization would \[thus\] have vastly improved survival prospects since it would be guided by superintelligent foresight and planning.”

Tallinn makes the same point during a Future of Life Institute podcast recorded this year. Whereas a runaway climate scenario is at best many decades away, if it could happen at all, Tallinn [speculates](https://futureoflife.org/2021/04/20/jaan-tallinn-on-avoiding-civilizational-pitfalls-and-surviving-the-21st-century/) that superintelligence will present “an existential risk in the next 10 or 50 years.” Thus, he says, “if you’re going to really get AI right \[by making it ‘friendly’\], it seems like all the other risks \[that we might face\] become much more manageable.” This is about as literal an interpretation of “_deus ex machina_” as one can get, and in my experience as someone who spent several months as a visiting scholar at the Centre for the Study of Existential Risk, which was cofounded by Tallinn, it’s a widely-held view among longtermists. In fact, Greaves and MacAskill [estimate](https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf) that every $100 spent on creating a “friendly” superintelligence would be morally equivalent to “saving one trillion \[actual human\] lives,” assuming that an additional 10<sup>24</sup> people could come to exist in the far future. Hence, they point out that focusing on superintelligence gets you a _way bigger_ bang for your buck than, say, preventing people who exist right now from contracting malaria by distributing [mosquito nets](https://en.wikipedia.org/wiki/Mosquito_net).

___

What I find most unsettling about the longtermist ideology isn’t just that it contains all the ingredients necessary for a [genocidal catastrophe](https://www.google.com/search?q=haggstrom+%22you+must+be+willing+to+break+a+few+eggs,%E2%80%9D+which+has+typically+been+used+to+explain+that+a%22&tbm=bks&sxsrf=ALeKk00ecCa6cZZNkMrgVBD_IttGPwKLVg:1627464586070&ei=iiMBYbXnA4S3gwfippaQBw&oq=haggstrom+%22you+must+be+willing+to+break+a+few+eggs,%E2%80%9D+which+has+typically+been+used+to+explain+that+a%22&gs_l=psy-ab.3...7009.7400.0.7668.3.3.0.0.0.0.144.247.0j2.2.0....0...1c.1j2.64.psy-ab..1.0.0....0.8Fa7eTgyzKA) in the name of realizing astronomical amounts of far-future “value.” Nor is it that this religious ideology has already infiltrated the consciousness of powerful actors who could, [for example](https://www.globalcitizen.org/en/content/billionaires-bezos-branson-musk-space-world-hunger/), “save 41 \[million\] people at risk of starvation” but instead use their wealth to [fly themselves to space](https://www.theguardian.com/science/2021/jul/19/billionaires-space-tourism-environment-emissions). Even more chilling is that many people in the community believe that their [mission to](https://www.google.com/search?q=%2522The+challenge+of+our+time+is+to+preserve+our+vast+potential,+and+to+protect+it+against+the+risk+of+future+destruction%2522&tbm=bks&sxsrf=ALeKk030oOJpgYZ9W7TnVS0jbg_cwv1mhQ:1626553426870&ei=UjzzYInJNIm4sAfB24f4CA&oq=%2522The+challenge+of+our+time+is+to+preserve+our+vast+potential,+and+to+protect+it+against+the+risk+of+future+destruction%2522&gs_l=psy-ab.3...5781.5781.0.6014.1.1.0.0.0.0.0.0..0.0....0...1c.1.64.psy-ab..1.0.0....0.JjOFq9K0BOY) “protect” and “preserve” humanity’s “longterm potential” is _so important_ that they have little tolerance for dissenters. These include critics who might suggest that longtermism is dangerous, or that it supports what Frances Lee Ansley calls [white supremacy](https://www.theatlantic.com/politics/archive/2017/10/the-language-of-white-supremacy/542148/) (given the implication, outlined and defended by Beckstead, that we should prioritize the lives of people in rich countries). When one believes that _existential risk_ is the most important concept ever invented, as someone at the Future of Humanity Institute once told me, and that failing to realize “our potential” would not merely be _wrong_ but a moral catastrophe of literally _cosmic proportions_, one will naturally be inclined to react strongly against those who criticize this sacred dogma. When you believe the stakes are that high, you may be quite willing to use extraordinary means to stop anyone who stands in your way. 

> _By reducing morality to an abstract numbers game, and by declaring that what’s most important is fulfilling “our potential” by becoming simulated posthumans among the stars, longtermists not only trivialize past atrocities like WWII (and the Holocaust) but give themselves a “moral excuse” to dismiss or minimize comparable atrocities in the future._

In fact, numerous people have come forward, both publicly and privately, over the past few years with stories of being intimidated, silenced, or “canceled.” (Yes, “cancel culture” is a real problem here.) I personally have had three colleagues back out of collaborations with me after I self-published a [short critique of longtermism](https://c8df8822-f112-4676-8332-ad89713358e3.filesusr.com/ugd/d9aaad_89094654cf0945738f5633b5d46653fd.pdf), not because they wanted to, but because they were pressured to do so from longtermists in the community. Others have expressed worries about the personal repercussions of openly criticizing Effective Altruism or the longtermist ideology. For example, the moral philosopher Simon Knutsson wrote a [critique](https://www.simonknutsson.com/problems-in-effective-altruism-and-existential-risk-and-what-to-do-about-them/) several years ago in which he notes, among other things, that Bostrom appears to have repeatedly misrepresented his academic achievements in claiming that, as he [wrote on his website](https://web.archive.org/web/20060701201235/https://nickbostrom.com/) in 2006, “my performance as an undergraduate set a national record in Sweden.” (There is no evidence that this is true.) The point is that, after doing this, Knutsson [reports](https://www.simonknutsson.com/problems-in-effective-altruism-and-existential-risk-and-what-to-do-about-them/%23Writing_this_text) that he became “concerned about his safety” given past efforts to censure certain ideas by longtermists with clout in the community.

This might sound hyperbolic, but it’s consistent with a pattern of questionable behavior from leaders in the Effective Altruism movement more generally. For example, one of the first people to become an Effective Altruist after the movement was born circa 2009, Simon Jenkins, reports an incident in which he criticized an idea within Effective Altruism on a Facebook group run by the community. Within an hour, not only had his post been deleted but someone who works for the Centre for Effective Altruism actually _called his personal phone_ to instruct him not to question the movement. “We can’t have people posting anything that suggests that Giving What We Can \[an organization founded by Ord\] is bad,” as Jenkins recalls. These are just a few of several dozen stories that people have shared with me after I went public with some of my own unnerving experiences.

All of this is to say that I’m not especially optimistic about convincing longtermists that their obsession with our “vast and glorious” potential ([quoting](https://www.google.com/search?tbm=bks&q=%2522vast+and+glorious%2522+%2522the+precipice%2522) Ord again) could have profoundly harmful consequences if it were to guide actual policy in the world. As the Swedish scholar Olle Häggström has [disquietingly noted](https://www.google.com/search?tbm=bks&q=%2522It+is+simply+too+reminiscent+of+the+old+saying+%25E2%2580%259CIf+you+want+to+make+an+omelet,+you+must+be+willing+to+break+a+few+eggs,%25E2%2580%259D+which+has+typically+been+used+to+explain+that+a+bit+of+genocide+or+so+might+be+a+good+thing,+if+it+can+con-+tribute+to+the+goal+of+creating+a+future+utopia.%2522), if political leaders were to take seriously the claim that saving billions of living, breathing, actual people today is morally equivalent to _negligible_ reductions in existential risk, who knows what atrocities this might excuse? If the ends justify the means, and the “end” in this case is a veritable techno-Utopian playground full of 10<sup>58</sup> simulated posthumans awash in “the pulsing ecstasy of love,” as Bostrom writes in his grandiloquent “[Letter from Utopia](https://www.nickbostrom.com/utopia.html),” would _any_ means be off-limits? While [some](https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf) longtermists have recently suggested that there should be constraints on which actions we can take for the far future, others like Bostrom have literally argued that [preemptive violence](https://nickbostrom.com/existential/risks.html) and even a [global surveillance system](https://www.nickbostrom.com/papers/vulnerable.pdf) should remain options for ensuring the realization of “our potential.” It’s not difficult to see how this way of thinking could have genocidally catastrophic consequences if political actors were to “\[take\] Bostrom’s argument to heart,” in Häggström’s [words](https://www.google.com/search?tbm=bks&q=%2522if+the+president+has+taken+Bostrom%27s+argument+to+heart%2522).

___

I should emphasize that rejecting longtermism does _not_ mean that one must reject _long-term thinking_. You _ought to_ care equally about people no matter when they exist, whether today, next year, or in a couple billion years henceforth. If we shouldn’t discriminate against people based on their spatial distance from us, we shouldn’t discriminate against them based on their temporal distance, either. Many of the problems we face today, such as climate change, will have devastating consequences for future generations hundreds or thousands of years in the future. That should matter. We should be willing to make sacrifices for their wellbeing, just as we make sacrifices for those alive today by donating to charities that fight global poverty. But this does not mean that one must genuflect before the altar of “future value” or “our potential,” understood in techno-Utopian terms of colonizing space, becoming posthuman, subjugating the natural world, maximizing economic productivity, and creating massive computer simulations stuffed with 10<sup>45</sup> digital beings (on Greaves and MacAskill’s [estimate](https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf) if we were to colonize the Milky Way).

Care about the long term, I like to say, but don’t be a _longtermist._ Superintelligent machines aren’t going to save us, and climate change _really should_ be one of our top global priorities, _whether or not_ it prevents us from becoming simulated posthumans in cosmic computers.

Although a handful of longtermists have recently written that the Effective Altruism movement should take climate change more seriously, among the main reasons given for doing so is that, to [quote](https://forum.effectivealtruism.org/posts/BwDAN9pGbmCYZGbgf/does-climate-change-deserve-more-attention-within-ea) an employee at the Centre for Effective Altruism, “by failing to show a sufficient appreciation of the severity of climate change, EA may risk losing credibility and alienating potential effective altruists.” In other words, community members should talk more about climate change not because of moral considerations relating to climate justice, the harms it will cause to poor people, and so on, but for _marketing reasons_. It would be “bad for business” if the public were to associate a dismissive attitude about climate change with Effective Altruism and its longtermist offshoot. As the same author reiterates later on, “I agree \[with Bostrom, Ord, etc.\] that it is much more important to work on x-risk … , but I wonder whether we are alienating potential EAs by not grappling with this issue.”

Yet even if longtermists were to come around to “caring” about climate change, this wouldn’t mean much if it were for the wrong reasons. Knutsson says:

> _“Like politicians, one cannot simply and naively assume that these people are being honest about their views, wishes, and what they would do. In the Effective Altruism and existential risk areas, some people seem super-strategic and willing to say whatever will achieve their goals, regardless of whether they believe the claims they make—even more so than in my experience of party politics.”_ 

Either way, the damage may already have been done, given that averting “untold suffering” from climate change will require _immediate action_ from the Global North. Meanwhile, millionaires and billionaires under the influence of longtermist thinking are focused instead on superintelligent machines that they believe will magically solve the mess that, in [large part](https://www.oxfam.org/en/press-releases/carbon-emissions-richest-1-percent-more-double-emissions-poorest-half-humanity), they themselves have created.
